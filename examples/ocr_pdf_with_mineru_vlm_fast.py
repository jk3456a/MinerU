import copy
import glob
import os
import random
import traceback

import json

import time
import uuid

from mineru.backend.vlm.vlm_analyze import doc_analyze as vlm_doc_analyze
from mineru.backend.vlm.predictor import get_predictor
from mineru.data.data_reader_writer import FileBasedDataWriter

# ==================== é…ç½®å‚æ•° ====================
# VLMç›¸å…³é…ç½®
DEFAULT_LANG = "ch"  # é»˜è®¤è¯­è¨€
FORMULA_ENABLE = True  # æ˜¯å¦å¯ç”¨å…¬å¼è¯†åˆ«
TABLE_ENABLE = True  # æ˜¯å¦å¯ç”¨è¡¨æ ¼è¯†åˆ«

PROJECT_DIR = "/cache/lizhen/repos/MinerU"

# è·¯å¾„é…ç½®
DEFAULT_SAVE_DIR = PROJECT_DIR + "/output_vlm"  # VLMç»“æœä¿å­˜ç›®å½•
LIBGEN_PDF_DIR = PROJECT_DIR + "/input/sample_pdf_300"  # PDFæºæ–‡ä»¶ç›®å½•
PDF_LIST_FILE = PROJECT_DIR + "/input/pdf_list.txt"  # å¾…å¤„ç†PDFåˆ—è¡¨æ–‡ä»¶
LOCAL_IMAGE_TMP_DIR = PROJECT_DIR + "/tmp/images_vlm"  # ä¸´æ—¶å›¾ç‰‡ç›®å½•

# æµ‹è¯•ä»»åŠ¡è·¯å¾„é…ç½®
TEST_PDF_DIR = "/cache/lizhen/repos/MinerU/input/sample_pdf_300/*pdf"  # æµ‹è¯•PDFç›®å½•
TEST_SAVE_DIR = "/cache/lizhen/repos/MinerU/output_vlm"  # æµ‹è¯•ç»“æœä¿å­˜ç›®å½•

# VLMåç«¯é…ç½®
VLM_BACKEND = "sglang-engine"  # å¯é€‰: "sglang-engine", "sglang-client", "transformers"
# VLM_MODEL_PATH = "opendatalab/MinerU2.0-2505-0.9B"  # HuggingFace
VLM_MODEL_PATH = "OpenDataLab/MinerU2.0-2505-0.9B"  # ModelScope
SGLANG_SERVER_URL = "http://localhost:30000/v1/chat/completions"  # sglang-clientæ—¶ä½¿ç”¨

# ç¯å¢ƒå˜é‡é…ç½®
# MODEL_SOURCE = "huggingface"  # å¦‚æœè¦ä»HuggingFaceä¸‹è½½ï¼Œä½¿ç”¨"huggingface"
MODEL_SOURCE = "modelscope"  # å¦‚æœè¦ä»ModelScopeä¸‹è½½ï¼Œä½¿ç”¨"modelscope"
VIRTUAL_VRAM_SIZE = "24"  # è™šæ‹Ÿæ˜¾å­˜å¤§å°(GB)

# è®¾å¤‡é…ç½®
DEVICE = "cuda"  # è¿è¡Œè®¾å¤‡

# æµ‹è¯•æ¨¡å¼é…ç½®
TEST_MODE = False  # æ˜¯å¦å¼€å¯æµ‹è¯•æ¨¡å¼ï¼ˆåªæµ‹è¯•1ä¸ªPDFï¼‰
TEST_SINGLE_PDF = False  # å•PDFæµ‹è¯•æ¨¡å¼

# VLMæ€§èƒ½ä¼˜åŒ–å‚æ•°
# VLM_PERFORMANCE_CONFIG = {
#     "temperature": 0.0,
#     "top_p": 0.9,
#     "top_k": 50,
#     "max_new_tokens": 4096,
#     "repetition_penalty": 1.0,
#     "presence_penalty": 0.0,
#     # SGLangæ€§èƒ½ä¼˜åŒ–å‚æ•°
#     "enable_torch_compile": True,  # å¯ç”¨torch compileçº¦15%åŠ é€Ÿ
#     "tp_size": 1,  # å¼ é‡å¹¶è¡Œï¼Œå¤šå¡æ—¶å¯å¢åŠ 
#     "dp_size": 1,  # æ•°æ®å¹¶è¡Œï¼Œå¤šå¡æ—¶å¯å¢åŠ 
#     "mem_fraction_static": 0.7,  # KVç¼“å­˜å¤§å°ï¼Œæ˜¾å­˜ä¸è¶³æ—¶é™ä½
# }

# å…¨å±€predictorç¼“å­˜
_global_predictor = None


def get_vlm_predictor():
    """è·å–VLMé¢„æµ‹å™¨ï¼Œä½¿ç”¨å…¨å±€ç¼“å­˜é¿å…é‡å¤åˆå§‹åŒ–"""
    global _global_predictor
    
    if _global_predictor is None:
        print("åˆå§‹åŒ–VLMé¢„æµ‹å™¨...")
        start_time = time.time()
        
        if VLM_BACKEND == "sglang-engine":
            # ä½¿ç”¨sglang-engineåç«¯ï¼ˆæœ¬åœ°æ¨ç†ï¼Œæœ€å¿«ï¼‰
            _global_predictor = get_predictor(
                backend="sglang-engine",
                model_path=VLM_MODEL_PATH,
                # **VLM_PERFORMANCE_CONFIG
            )
        elif VLM_BACKEND == "sglang-client":
            # ä½¿ç”¨sglang-clientåç«¯ï¼ˆè¿æ¥sglang-serverï¼‰
            _global_predictor = get_predictor(
                backend="sglang-client",
                server_url=SGLANG_SERVER_URL,
                # temperature=VLM_PERFORMANCE_CONFIG["temperature"],
                # top_p=VLM_PERFORMANCE_CONFIG["top_p"],
                # top_k=VLM_PERFORMANCE_CONFIG["top_k"],
                # max_new_tokens=VLM_PERFORMANCE_CONFIG["max_new_tokens"],
                # repetition_penalty=VLM_PERFORMANCE_CONFIG["repetition_penalty"],
                # presence_penalty=VLM_PERFORMANCE_CONFIG["presence_penalty"],
            )
        elif VLM_BACKEND == "transformers":
            # ä½¿ç”¨transformersåç«¯ï¼ˆå…¼å®¹æ€§æœ€å¥½ä½†è¾ƒæ…¢ï¼‰
            _global_predictor = get_predictor(
                backend="transformers",
                model_path=VLM_MODEL_PATH,
                # temperature=VLM_PERFORMANCE_CONFIG["temperature"],
                # top_p=VLM_PERFORMANCE_CONFIG["top_p"],
                # top_k=VLM_PERFORMANCE_CONFIG["top_k"],
                # max_new_tokens=VLM_PERFORMANCE_CONFIG["max_new_tokens"],
                # repetition_penalty=VLM_PERFORMANCE_CONFIG["repetition_penalty"],
                # presence_penalty=VLM_PERFORMANCE_CONFIG["presence_penalty"],
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„VLMåç«¯: {VLM_BACKEND}")
        
        init_time = time.time() - start_time
        print(f"VLMé¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
    
    return _global_predictor


def infer_one_pdf_vlm(pdf_file_path, lang=DEFAULT_LANG):
    """ä½¿ç”¨VLMåç«¯å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
    t0 = time.time()
    
    # è¯»å–PDFæ–‡ä»¶
    with open(pdf_file_path, 'rb') as fi:
        pdf_bytes = fi.read()
    t1 = time.time()
    print(f"è¯»å–PDFæ–‡ä»¶è€—æ—¶: {t1 - t0:.3f}ç§’")
    
    # è·å–VLMé¢„æµ‹å™¨
    predictor = get_vlm_predictor()
    t2 = time.time()
    print(f"è·å–é¢„æµ‹å™¨è€—æ—¶: {t2 - t1:.3f}ç§’")
    
    # è®¾ç½®å›¾ç‰‡ä¿å­˜ç›®å½•
    pdf_name = os.path.basename(pdf_file_path)
    local_image_dir = f"{LOCAL_IMAGE_TMP_DIR}/{pdf_name}"
    if not os.path.exists(local_image_dir):
        os.makedirs(local_image_dir, exist_ok=True)
    image_writer = FileBasedDataWriter(local_image_dir)
    
    # ä½¿ç”¨VLMåç«¯è¿›è¡Œæ–‡æ¡£åˆ†æ
    print(f"å¼€å§‹VLMåˆ†æï¼Œåç«¯: {VLM_BACKEND}")
    middle_json, vlm_results = vlm_doc_analyze(
        pdf_bytes,
        image_writer=image_writer,
        predictor=predictor,
        backend=VLM_BACKEND,
        # å¯ä»¥ä¼ é€’é¢å¤–çš„VLMå‚æ•°
        formula_enable=FORMULA_ENABLE,
        table_enable=TABLE_ENABLE,
    )
    t3 = time.time()
    print(f"VLMæ–‡æ¡£åˆ†æè€—æ—¶: {t3 - t2:.3f}ç§’")
    
    # æ„é€ ç»“æœ
    vlm_result = {
        "middle_json": middle_json,
        "vlm_results": vlm_results,
        "backend": VLM_BACKEND,
        "model_path": VLM_MODEL_PATH,
        # "performance_config": VLM_PERFORMANCE_CONFIG
    }
    
    total_time = t3 - t0
    analysis_percent = (t3 - t2) / total_time * 100
    print(f"æ€»è€—æ—¶: {total_time:.3f}ç§’, VLMåˆ†æå æ¯”: {analysis_percent:.1f}%")
    
    return vlm_result


def process_one_pdf_file(pdf_path, save_dir=None, lang=DEFAULT_LANG):
    """å¤„ç†å•ä¸ªPDFæ–‡ä»¶ï¼Œä¿æŒåŸæœ‰é€»è¾‘ä¸å˜"""
    if not save_dir:
        save_dir = DEFAULT_SAVE_DIR
    
    pdf_file_name = os.path.basename(pdf_path)
    target_file = f"{save_dir}/{pdf_file_name}.json"  # æ·»åŠ .jsonåç¼€ä¾¿äºåŒºåˆ†
    
    if not os.path.exists(save_dir):
        os.makedirs(save_dir, exist_ok=True)
    
    # æ£€æŸ¥ç»“æœæ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(target_file):
        print(f"VLMç»“æœå·²å­˜åœ¨: [{target_file}]")
        return
    
    try:
        # ä½¿ç”¨VLMåç«¯å¤„ç†
        print(f"å¼€å§‹å¤„ç†PDF: {pdf_file_name}")
        infer_result = infer_one_pdf_vlm(pdf_path, lang=lang)
        infer_result['pdf_path'] = pdf_path
        infer_result['processing_time'] = time.time()
        
        # ä¿å­˜ç»“æœ
        res_json_str = json.dumps(infer_result, ensure_ascii=False, indent=2)
        with open(target_file, "w", encoding='utf-8') as fo:
            fo.write(res_json_str)
        
        print(f"VLMå¤„ç†å®Œæˆ: {target_file}")
        
    except Exception as e:
        print(f"å¤„ç†PDFå¤±è´¥: {pdf_file_name}, é”™è¯¯: {str(e)}")
        traceback.print_exc()


def get_all_access_pdf_paths():
    """è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„PDFè·¯å¾„ï¼Œä¿æŒåŸæœ‰é€»è¾‘"""
    print(glob.glob("/*"))
    pdf_files = glob.glob(LIBGEN_PDF_DIR + "/*pdf")  # ä¿®æ­£è·¯å¾„
    print(f"æ‰¾åˆ°PDFæ–‡ä»¶: {len(pdf_files)}, å‰10ä¸ª: {pdf_files[:10]}")
    
    if not os.path.exists(PDF_LIST_FILE):
        print(f"PDFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {PDF_LIST_FILE}, è¿”å›æ‰€æœ‰PDFæ–‡ä»¶")
        random.shuffle(pdf_files)
        return pdf_files
    
    book_names = set()
    try:
        with open(PDF_LIST_FILE, encoding='utf-8') as fi:
            content = fi.read().strip()
            if content:
                for x in eval(content):
                    book_name = os.path.basename(x)
                    book_names.add(book_name)
    except Exception as e:
        print(f"è¯»å–PDFåˆ—è¡¨æ–‡ä»¶å¤±è´¥: {e}, è¿”å›æ‰€æœ‰PDFæ–‡ä»¶")
        random.shuffle(pdf_files)
        return pdf_files
    
    pdf_to_process_list = []
    for x in pdf_files:
        if os.path.basename(x) in book_names:
            pdf_to_process_list.append(x)
    
    random.shuffle(pdf_to_process_list)
    print(f"æ ¹æ®åˆ—è¡¨ç­›é€‰åçš„PDFæ–‡ä»¶æ•°: {len(pdf_to_process_list)}")
    return pdf_to_process_list


def run_test_task():
    """è¿è¡Œæµ‹è¯•ä»»åŠ¡ï¼Œä¿æŒåŸæœ‰é€»è¾‘"""
    pdf_files = glob.glob(TEST_PDF_DIR)
    save_dir = TEST_SAVE_DIR
    
    print("=" * 80)
    print("MinerU VLMé«˜é€Ÿåç«¯æ€§èƒ½æµ‹è¯•")
    print(f"åç«¯: {VLM_BACKEND}")
    print("=" * 80)
    
    if TEST_MODE and TEST_SINGLE_PDF:
        # æµ‹è¯•æ¨¡å¼ï¼šåªæµ‹è¯•ç¬¬ä¸€ä¸ªPDF
        if pdf_files:
            pdf_files = pdf_files[:1]
            print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼ï¼šåªæµ‹è¯•1ä¸ªPDFæ–‡ä»¶")
        else:
            print("âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return
    
    print(f"å¾…æµ‹è¯•PDFæ–‡ä»¶æ•°é‡: {len(pdf_files)}")
    if pdf_files:
        test_file = pdf_files[0]
        file_size = os.path.getsize(test_file) / 1024 / 1024  # MB
        print(f"æµ‹è¯•æ–‡ä»¶: {os.path.basename(test_file)} ({file_size:.2f} MB)")
    
    # é¢„åˆå§‹åŒ–VLMé¢„æµ‹å™¨
    print(f"\nğŸš€ åˆå§‹åŒ–{VLM_BACKEND}é¢„æµ‹å™¨...")
    init_start_time = time.time()
    try:
        get_vlm_predictor()
        init_time = time.time() - init_start_time
        print(f"âœ… é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ VLMé¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®: 1) æ£€æŸ¥æ˜¾å­˜æ˜¯å¦å……è¶³ 2) å°è¯•ä½¿ç”¨transformersåç«¯")
        return
    
    success_count = 0
    failed_count = 0
    total_start_time = time.time()
    
    for idx, file_path in enumerate(pdf_files):
        if not os.path.exists(file_path):
            continue
        
        print(f"\nğŸ“„ å¤„ç†è¿›åº¦: {idx + 1}/{len(pdf_files)} - {os.path.basename(file_path)}")
        
        try:
            start_time = time.time()
            process_one_pdf_file(file_path, save_dir)
            process_time = time.time() - start_time
            print(f"âœ… VLMå¤„ç†è€—æ—¶: {process_time:.2f}ç§’")
            success_count += 1
            
            # æµ‹è¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½ä¿¡æ¯
            if TEST_MODE:
                throughput = file_size / process_time if 'file_size' in locals() else 0
                print(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {throughput:.2f} MB/s")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            print('=====' * 10)
            failed_count += 1
    
    total_time = time.time() - total_start_time
    print(f"\n" + "=" * 80)
    print(f"VLMåç«¯æµ‹è¯•å®Œæˆ!")
    print(f"åç«¯: {VLM_BACKEND}")
    print(f"æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ (å«åˆå§‹åŒ–: {init_time:.2f}ç§’)")
    if success_count > 0 and 'file_size' in locals():
        avg_throughput = file_size * success_count / total_time
        print(f"å¹³å‡å¤„ç†é€Ÿåº¦: {avg_throughput:.2f} MB/s")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°ï¼Œä¿æŒåŸæœ‰é€»è¾‘"""
    pdf_files = get_all_access_pdf_paths()
    print(f"ä¸»ä»»åŠ¡PDFæ–‡ä»¶æ•°é‡: {len(pdf_files)}")
    
    # é¢„åˆå§‹åŒ–VLMé¢„æµ‹å™¨
    try:
        get_vlm_predictor()
    except Exception as e:
        print(f"VLMé¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…sglang: pip install mineru[all]")
        return
    
    success_count = 0
    failed_count = 0
    
    for idx, file_path in enumerate(pdf_files):
        if not os.path.exists(file_path):
            continue
        
        print(f"\nå¤„ç†è¿›åº¦: {idx + 1}/{len(pdf_files)} - {os.path.basename(file_path)}")
        
        try:
            start_time = time.time()
            process_one_pdf_file(file_path)
            process_time = time.time() - start_time
            print(f"å¤„ç†è€—æ—¶: {process_time:.2f}ç§’")
            success_count += 1
            
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            print('=====' * 10)
            failed_count += 1
    
    print(f"\nå¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")


def batch_process_pdfs(pdf_paths, batch_size=1):
    """æ‰¹é‡å¤„ç†PDFæ–‡ä»¶ï¼ˆVLMæš‚ä¸æ”¯æŒçœŸæ­£çš„æ‰¹å¤„ç†ï¼Œä½†å¯ä»¥ä¼˜åŒ–é¢„æµ‹å™¨å¤ç”¨ï¼‰"""
    print(f"æ‰¹é‡å¤„ç† {len(pdf_paths)} ä¸ªPDFæ–‡ä»¶ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # é¢„åˆå§‹åŒ–VLMé¢„æµ‹å™¨
    get_vlm_predictor()
    
    for i in range(0, len(pdf_paths), batch_size):
        batch = pdf_paths[i:i + batch_size]
        print(f"\nå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}: {len(batch)} ä¸ªæ–‡ä»¶")
        
        for pdf_path in batch:
            if os.path.exists(pdf_path):
                try:
                    process_one_pdf_file(pdf_path)
                except Exception as e:
                    print(f"æ‰¹é‡å¤„ç†å¤±è´¥: {pdf_path}, é”™è¯¯: {e}")


if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["MINERU_MODEL_SOURCE"] = MODEL_SOURCE  # è®¾ç½®æ¨¡å‹æº
    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = VIRTUAL_VRAM_SIZE
    
    # SGLangæ€§èƒ½ä¼˜åŒ–ç¯å¢ƒå˜é‡
    if VLM_BACKEND.startswith("sglang"):
        # å¯ç”¨torchç¼–è¯‘åŠ é€Ÿ
        # if VLM_PERFORMANCE_CONFIG.get("enable_torch_compile", False):
        os.environ["SGLANG_TORCH_COMPILE"] = "1"
    
    print("=" * 60)
    print("MinerU VLMé«˜é€Ÿæµ‹è¯•è„šæœ¬")
    print(f"åç«¯: {VLM_BACKEND}")
    print(f"æ¨¡å‹: {VLM_MODEL_PATH}")
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"è™šæ‹Ÿæ˜¾å­˜: {VIRTUAL_VRAM_SIZE}GB")
    print("=" * 60)
    
    start_time = time.time()
    
    # å¯ä»¥é€‰æ‹©è¿è¡Œæµ‹è¯•ä»»åŠ¡æˆ–ä¸»ä»»åŠ¡
    run_test_task()  # è¿è¡Œæµ‹è¯•ä»»åŠ¡
    # main()  # è¿è¡Œä¸»ä»»åŠ¡
    
    total_time = time.time() - start_time
    print(f'\næ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’')
    print("VLMé«˜é€Ÿå¤„ç†å®Œæˆï¼")